# 实验任务一：vLLM推理框架使用

[测试链接](https://cdn.jsdelivr.net/gh/zhiweinju/nju-dl-lab-2025spring@main/docs/lab1/lab1.ipynb)

## LLM推理原理基础

LLM（Large Language Model）推理是指使用预训练的语言模型进行文本生成、理解等任务的过程。LLM通常具有数十亿甚至数千亿的参数，能够处理复杂的自然语言任务。

LLM的基本原理：

1. **模型架构**：LLM通常基于Transformer架构，使用自注意力机制来捕捉文本中的长距离依赖关系。

2. **预训练与微调**：LLM通常先在大规模文本数据上进行预训练，然后在特定任务上进行微调，以提高模型的性能。

3. **推理过程**：在推理阶段，模型接收输入文本，通过前向传播计算输出，生成预测结果。

4. **解码策略**：在生成文本时，常用的解码策略包括贪婪搜索、束搜索（Beam Search）和采样等，这些策略决定了生成文本的多样性和质量。

## LLM推理框架vLLM简介

vLLM是一个高性能的推理框架，旨在提供快速、可扩展的LLM推理服务。它支持丰富的模型，并提供了易于使用的API。

**目标**
本次实验是xxxxxx，实验目标为：

1. 使用vLLM部署Qwen2-7B模型

2. xxx。

------

### **1. 环境准备**

安装vLLM，请参考[vLLM安装指南](https://vllm.ai)

